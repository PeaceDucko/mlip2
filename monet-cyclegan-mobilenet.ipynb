{"cells":[{"metadata":{},"cell_type":"markdown","source":["# Introduction and Setup\n","\n","This notebook utilizes a CycleGAN architecture to add Monet-style to photos. For this tutorial, we will be using the TFRecord dataset. Import the following packages and change the accelerator to TPU.\n","\n","For more information, check out [TensorFlow](https://www.tensorflow.org/tutorials/generative/cyclegan) and [Keras](https://keras.io/examples/generative/cyclegan/) CycleGAN documentation pages."]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","from kaggle_datasets import KaggleDatasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from scipy import linalg\n","\n","%matplotlib inline\n","\n","import os, re\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    \n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Load in the data\n","\n","We want to keep our photo dataset and our Monet dataset separate. First, load in the filenames of the TFRecords."]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Initialize global variables\n","IMAGE_SIZE = [256, 256]\n","EPOCHS = 28\n","BATCH_SIZE = 4\n","\n","GCS_PATH = KaggleDatasets().get_gcs_path()\n","\n","# Counting the number of items within the monet and photo ds. This wil later be used to calculate the steps per epoch\n","def count_data_items(filenames):\n","    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n","    return np.sum(n)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["MONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\n","print('Monet TFRecord Files:', len(MONET_FILENAMES))\n","\n","PHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))\n","print('Photo TFRecord Files:', len(PHOTO_FILENAMES))\n","\n","n_monet_samples = count_data_items(MONET_FILENAMES)\n","n_photo_samples = count_data_items(PHOTO_FILENAMES)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["All the images for the competition are already sized to 256x256. As these images are RGB images, set the channel to 3. Additionally, we need to scale the images to a [-1, 1] scale. Because we are building a generative model, we don't need the labels or the image id so we'll only return the image from the TFRecord."]},{"metadata":{},"cell_type":"markdown","source":["Define the function to extract the image from the files."]},{"metadata":{"trusted":true},"cell_type":"code","source":["def load_dataset(filenames, labeled=True, ordered=False):\n","    dataset = tf.data.TFRecordDataset(filenames)\n","    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n","    return dataset"],"execution_count":null,"outputs":[]},{"source":["# Augmentations\n","Data augmentation for GANs should be done very carefully, especially for tasks similar to style transfer, if we apply transformations that can change too much the style of the data (e.g. brightness, contrast, saturation) it can cause the generator to do not efficiently learn the base style, so in this case, we are using only spatial transformations like, flips, rotates and crops."],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["def get_gan_dataset(files, augment=None, repeat=True, shuffle=True, batch_size=1):\n","\n","    ds = load_dataset(files) # Load the Monet images\n","    \n","    # If an augment function is passed, apply the augmentation to the images \n","    if augment:\n","        ds = ds.map(lambda p: augment(p, True), num_parallel_calls=AUTOTUNE)\n","        \n","    #If repeat is set to True, apply to the images\n","    if repeat:\n","        ds = ds.repeat()\n","        \n","    #If shuffle is set to True, apply to the images\n","    if shuffle:\n","        ds = ds.shuffle(2048)\n","    \n","    # After the data has been augmented, repeated, and shuffled, then\n","    # apply the batch size of 4\n","    ds = ds.batch(batch_size, drop_remainder=True)\n","    ds = ds.cache()\n","    ds = ds.prefetch(AUTOTUNE)\n","        \n","    return ds\n","\n","\n","def decode_image(image):\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = (tf.cast(image, tf.float32) / 127.5) - 1\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image\n","\n","def read_tfrecord(example):\n","    tfrecord_format = {\n","        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n","        \"image\": tf.io.FixedLenFeature([], tf.string),\n","        \"target\": tf.io.FixedLenFeature([], tf.string)\n","    }\n","    example = tf.io.parse_single_example(example, tfrecord_format)\n","    image = decode_image(example['image'])\n","    return image\n","\n","def _gaussian_kernel(kernel_size, sigma, n_channels, dtype):\n","    x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=dtype)\n","    g = tf.math.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.cast(sigma, dtype), 2))))\n","    g_norm2d = tf.pow(tf.reduce_sum(g), 2)\n","    g_kernel = tf.tensordot(g, g, axes=0) / g_norm2d\n","    g_kernel = tf.expand_dims(g_kernel, axis=-1)\n","    return tf.expand_dims(tf.tile(g_kernel, (1, 1, n_channels)), axis=-1)\n","\n","def data_augment(image, real=False):\n","    \n","    # Randomly decide to rotate the image\n","    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    # Randomly decide to flip the image\n","    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    # Randomly decide to crop the image\n","    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n","    \n","    \n","    if p_crop > .5:\n","        image = tf.image.resize(image, [286, 286])\n","        image = tf.image.random_crop(image, size=[256, 256, 3])\n","        if p_crop > .9:\n","            image = tf.image.resize(image, [300, 300])\n","            image = tf.image.random_crop(image, size=[256, 256, 3])\n","    \n","    if p_rotate > .9:\n","        image = tf.image.rot90(image, k=3) # rotate 270ยบ\n","    elif p_rotate > .7:\n","        image = tf.image.rot90(image, k=2) # rotate 180ยบ\n","    elif p_rotate > .5:\n","        image = tf.image.rot90(image, k=1) # rotate 90ยบ\n","        \n","    if p_spatial > .6:\n","        image = tf.image.random_flip_left_right(image)\n","        image = tf.image.random_flip_up_down(image)\n","        if p_spatial > .9:\n","            image = tf.image.transpose(image)\n","            \n","    if real:\n","        blur = _gaussian_kernel(3, 2, 3, image.dtype)\n","        image = tf.nn.depthwise_conv2d(image[None], blur, [1,1,1,1], 'SAME')\n","        image = tf.reshape(image, [256, 256, 3])\n","    \n","    return image\n","\n","def photo_blur(image):\n","\n","    blur = _gaussian_kernel(3, 2, 3, image.dtype)\n","    image = tf.nn.depthwise_conv2d(image[None], blur, [1,1,1,1], 'SAME')\n","    image = tf.reshape(image, [256, 256, 3])\n","    \n","    return image"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's load in our datasets."]},{"metadata":{"trusted":true},"cell_type":"code","source":["monet_ds = get_gan_dataset(MONET_FILENAMES, augment=data_augment, repeat=False, shuffle=True, batch_size=BATCH_SIZE)\n","photo_ds = get_gan_dataset(PHOTO_FILENAMES, augment=data_augment, repeat=False, shuffle=True, batch_size=BATCH_SIZE)\n","\n","full_dataset = tf.data.Dataset.zip((monet_ds.repeat(), photo_ds.repeat()))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["example_monet , example_photo = next(iter(full_dataset))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's  visualize a photo example and a Monet example."]},{"metadata":{"trusted":true},"cell_type":"code","source":["sns.reset_orig()\n","\n","plt.subplot(121)\n","plt.title('Photo')\n","plt.imshow(example_photo[0] * 0.5 + 0.5)\n","\n","plt.subplot(122)\n","plt.title('Monet')\n","plt.imshow(example_monet[0] * 0.5 + 0.5)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Build the generator\n","\n","We'll be using a UNET architecture for our CycleGAN. To build our generator, let's first define our `downsample` and `upsample` methods.\n","\n","The `downsample`, as the name suggests, reduces the 2D dimensions, the width and height, of the image by the stride. The stride is the length of the step the filter takes. Since the stride is 2, the filter is applied to every other pixel, hence reducing the weight and height by 2.\n","\n","We'll be using an instance normalization instead of batch normalization. As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons."]},{"metadata":{"trusted":true},"cell_type":"code","source":["OUTPUT_CHANNELS = 3\n","\n","def downsample(filters, size, shape, apply_instancenorm=True):\n","    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","        \n","    inputs = keras.Input(shape=shape[1:])\n","\n","    x = layers.Conv2D(filters, size, strides=2,\n","                             padding='same',\n","                             kernel_initializer=initializer,\n","                             use_bias=False)(inputs)   \n","    \n","    if apply_instancenorm:\n","        x = tfa.layers.InstanceNormalization(gamma_initializer=initializer)(x)\n","\n","    x = layers.LeakyReLU()(x)\n","        \n","    model = keras.Model(inputs=inputs, outputs=x)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["`Upsample` does the opposite of downsample and increases the dimensions of the of the image. `Conv2DTranspose` does basically the opposite of a `Conv2D` layer."]},{"metadata":{"trusted":true},"cell_type":"code","source":["def upsample(filters, size, shape, apply_dropout=False):\n","    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    inputs = keras.Input(shape=shape[1:])\n","\n","    x = layers.Conv2DTranspose(filters, size, strides=2,\n","                                padding='same',\n","                                kernel_initializer=initializer,\n","                                use_bias=False)(inputs)\n","\n","    x = tfa.layers.InstanceNormalization(gamma_initializer=initializer)(x)\n","    \n","    if apply_dropout:\n","        x = layers.Dropout(0.5)(x)\n","        \n","    x = layers.ReLU()(x)\n","    model = keras.Model(inputs=inputs, outputs=x)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's build our generator!\n","\n","The generator first downsamples the input image and then upsample while establishing long skip connections. Skip connections are a way to help bypass the vanishing gradient problem by concatenating the output of a layer to multiple layers instead of only one. Here we concatenate the output of the downsample layer to the upsample layer in a symmetrical fashion."]},{"metadata":{"trusted":true},"cell_type":"code","source":["def Generator():\n","    inputs = layers.Input(shape=[256,256,3])\n","    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","    \n","    down_stack = [{\"filters\":64,\"size\":4,\"instancenorm\":False},\n","                       {\"filters\":128,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":256,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":512,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":512,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":512,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":512,\"size\":4,\"instancenorm\":True},\n","                       {\"filters\":512,\"size\":4,\"instancenorm\":True}]\n","\n","\n","    \n","    up_stack = [{\"filters\":512,\"size\":4,\"dropout\":True},\n","               {\"filters\":512,\"size\":4,\"dropout\":True},\n","               {\"filters\":512,\"size\":4,\"dropout\":True},\n","               {\"filters\":512,\"size\":4,\"dropout\":False},\n","               {\"filters\":256,\"size\":4,\"dropout\":False},\n","               {\"filters\":128,\"size\":4,\"dropout\":False},\n","               {\"filters\":64,\"size\":4,\"dropout\":False}]\n","\n","    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                  activation='tanh',\n","                                  strides=2,\n","                                  padding='same',\n","                                  kernel_initializer=initializer) # (bs, 256, 256, 3)\n","\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for obj in down_stack:\n","        x = downsample(obj['filters'],obj['size'],x.shape,obj['instancenorm'])(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for obj, skip in zip(up_stack, skips):\n","        x = upsample(obj['filters'],obj['size'],x.shape,obj['dropout'])(x)\n","        x = layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return keras.Model(inputs=inputs, outputs=x)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Build the discriminator\n","\n","The discriminator takes in the input image and classifies it as real or fake (generated). Instead of outputing a single node, the discriminator outputs a smaller 2D image with higher pixel values indicating a real classification and lower values indicating a fake classification."]},{"metadata":{"trusted":true},"cell_type":"code","source":["def Discriminator():\n","    inputs = layers.Input(shape=[256,256,3], name='input_image')\n","    initializer = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    x = inputs\n","\n","    downsample_stack = [{\"filters\":64,\"size\":4, 'instancenorm':False},\n","                       {\"filters\":128,\"size\":4, 'instancenorm':True},\n","                       {\"filters\":256,\"size\":4, 'instancenorm':True}]\n","\n","    for obj in downsample_stack:\n","        x = downsample(obj['filters'],obj['size'],x.shape,obj['instancenorm'])(x)\n","\n","    zero_pad1 = layers.ZeroPadding2D()(x) # (bs, 34, 34, 256)\n","    conv = layers.Conv2D(512, 4, strides=1,\n","                         kernel_initializer=initializer,\n","                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","    norm = tfa.layers.InstanceNormalization(gamma_initializer=initializer)(conv)\n","\n","    leaky_relu = layers.LeakyReLU()(norm)\n","\n","    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","\n","    last = layers.Conv2D(1, 4, strides=1,\n","                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=last)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    monet_generator = Generator() # transforms photos to Monet-esque paintings\n","    photo_generator = Generator() # transforms Monet paintings to be more like photos\n","\n","    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n","    photo_discriminator = Discriminator() # differentiates real photos and generated photos"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Since our generators are not trained yet, the generated Monet-esque photo does not show what is expected at this point."]},{"metadata":{"trusted":true},"cell_type":"code","source":["sns.reset_orig()\n","\n","to_monet = monet_generator(example_photo)\n","\n","plt.subplot(1, 2, 1)\n","plt.title(\"Original Photo\")\n","plt.imshow(example_photo[0] * 0.5 + 0.5)\n","\n","plt.subplot(1, 2, 2)\n","plt.title(\"Monet-esque Photo\")\n","plt.imshow(to_monet[0] * 0.5 + 0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"source":["# Initialize our Keras applications\n","\n","MobileNetV2 is used to help calculate the cycle consistency loss.\n","\n","InceptionV3 is used to help calculate the FID score.\n","\n","\n"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    cycle_model = tf.keras.applications.MobileNet(input_shape=(256,256,3),pooling=\"avg\",include_top=False)\n","    cycle_model.trainable = False\n","    \n","with strategy.scope():\n","    inception_model = tf.keras.applications.InceptionV3(input_shape=(256,256,3),pooling=\"avg\",include_top=False)\n","    inception_model.trainable = False"],"execution_count":null,"outputs":[]},{"source":["# FID metric\n","\n","The score summarizes how similar the two groups are in terms of statistics on computer vision features of the raw images calculated using the inception v3 model used for image classification. Lower scores indicate the two groups of images are more similar, or have more similar statistics, with a perfect score being 0.0 indicating that the two groups of images are identical."],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["def calculate_frechet_distance(mu_FID,sigma_FID,mu,sigma):\n","    fid_epsilon = 1e-14\n","    mu_FID = np.atleast_1d(mu_FID)\n","    mu = np.atleast_1d(mu)\n","    sigma_FID = np.atleast_2d(sigma_FID)\n","    sigma = np.atleast_2d(sigma)\n","\n","    assert mu_FID.shape == mu.shape, 'Training and test mean vectors have different lengths'\n","    assert sigma_FID.shape == sigma.shape, 'Training and test covariances have different dimensions'\n","\n","    # product might be almost singular\n","    covmean, _ = linalg.sqrtm(sigma_FID.dot(sigma), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = f'fid calculation produces singular product; adding {fid_epsilon} to diagonal of cov estimates'\n","        warnings.warn(msg)\n","        offset = np.eye(sigma_FID.shape[0]) * fid_epsilon\n","        covmean = linalg.sqrtm((sigma_FID + offset).dot(sigma + offset))\n","\n","    # numerical error might give slight imaginary component\n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError(f'Imaginary component {m}')\n","        covmean = covmean.real\n","    tr_covmean = np.trace(covmean)\n","    return (mu_FID - mu).dot(mu_FID - mu) + np.trace(sigma_FID) + np.trace(sigma) - 2 * tr_covmean\n","\n","def calculate_activation_statistics(images,fid_model):\n","        act=fid_model.predict(images)\n","        mu = np.mean(act, axis=0)\n","        sigma = np.cov(act, rowvar=False)\n","        return mu, sigma\n","    \n","mu, sigma = calculate_activation_statistics(monet_ds,inception_model)\n","\n","def FID(images,gen_model,inception_model=inception_model,mu=mu, sigma=sigma):\n","    with strategy.scope():\n","        inp = layers.Input(shape=[256, 256, 3], name='input_image')\n","        x = gen_model(inp)\n","        x = inception_model(x)\n","        fid_model = tf.keras.Model(inputs=inp, outputs=x)\n","\n","    mu_FID, sigma_FID = calculate_activation_statistics(images,fid_model)\n","\n","    fid_value = calculate_frechet_distance(mu_FID, sigma_FID, mu, sigma)\n","\n","    return fid_value"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Build the CycleGAN model\n","\n","We will subclass a `tf.keras.Model` so that we can run `fit()` later to train our model. During the training step, the model transforms a photo to a Monet painting and then back to a photo. The difference between the original photo and the twice-transformed photo is the cycle-consistency loss. We want the original photo and the twice-transformed photo to be similar to one another.\n","\n","The losses are defined in the next section."]},{"metadata":{"trusted":true},"cell_type":"code","source":["class CycleGan(keras.Model):\n","    def __init__(\n","        self,\n","        monet_generator,\n","        photo_generator,\n","        monet_discriminator,\n","        photo_discriminator,\n","        lambda_cycle=10,\n","    ):\n","        super(CycleGan, self).__init__()\n","        self.m_gen = monet_generator\n","        self.p_gen = photo_generator\n","        self.m_disc = monet_discriminator\n","        self.p_disc = photo_discriminator\n","        self.lambda_cycle = lambda_cycle\n","        \n","    def compile(\n","        self,\n","        m_gen_optimizer,\n","        p_gen_optimizer,\n","        m_disc_optimizer,\n","        p_disc_optimizer,\n","        gen_loss_fn,\n","        disc_loss_fn,\n","        cycle_loss_fn,\n","        identity_loss_fn\n","    ):\n","        super(CycleGan, self).compile()\n","        self.m_gen_optimizer = m_gen_optimizer\n","        self.p_gen_optimizer = p_gen_optimizer\n","        self.m_disc_optimizer = m_disc_optimizer\n","        self.p_disc_optimizer = p_disc_optimizer\n","        self.gen_loss_fn = gen_loss_fn\n","        self.disc_loss_fn = disc_loss_fn\n","        self.cycle_loss_fn = cycle_loss_fn\n","        self.identity_loss_fn = identity_loss_fn\n","        \n","    def train_step(self, batch_data):\n","        real_monet, real_photo = batch_data\n","        \n","        with tf.GradientTape(persistent=True) as tape:\n","            # photo to monet back to photo\n","            fake_monet = self.m_gen(real_photo, training=True)\n","            cycled_photo = self.p_gen(fake_monet, training=True)\n","\n","            # monet to photo back to monet\n","            fake_photo = self.p_gen(real_monet, training=True)\n","            cycled_monet = self.m_gen(fake_photo, training=True)\n","\n","            # generating itself\n","            same_monet = self.m_gen(real_monet, training=True)\n","            same_photo = self.p_gen(real_photo, training=True)\n","\n","            # discriminator used to check, inputing real images\n","            disc_real_monet = self.m_disc(real_monet, training=True)\n","            disc_real_photo = self.p_disc(real_photo, training=True)\n","\n","            # discriminator used to check, inputing fake images\n","            disc_fake_monet = self.m_disc(fake_monet, training=True)\n","            disc_fake_photo = self.p_disc(fake_photo, training=True)\n","\n","            # evaluates generator loss\n","            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n","            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n","\n","            # evaluates total cycle consistency loss\n","            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n","\n","            # evaluates total generator loss\n","            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n","            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n","\n","            # evaluates discriminator loss\n","            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n","            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n","\n","        # Calculate the gradients for generator and discriminator\n","        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n","                                                  self.m_gen.trainable_variables)\n","        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n","                                                  self.p_gen.trainable_variables)\n","\n","        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n","                                                      self.m_disc.trainable_variables)\n","        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n","                                                      self.p_disc.trainable_variables)\n","\n","        # Apply the gradients to the optimizer\n","        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n","                                                 self.m_gen.trainable_variables))\n","\n","        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n","                                                 self.p_gen.trainable_variables))\n","\n","        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n","                                                  self.m_disc.trainable_variables))\n","\n","        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n","                                                  self.p_disc.trainable_variables))\n","        \n","        return {\n","            \"monet_gen_loss\": total_monet_gen_loss,\n","            \"photo_gen_loss\": total_photo_gen_loss,\n","            \"monet_disc_loss\": monet_disc_loss,\n","            \"photo_disc_loss\": photo_disc_loss,\n","            \"cycle_loss\": total_cycle_loss\n","        }"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Define loss functions\n","\n","The discriminator loss function below compares real images to a matrix of 1s and fake images to a matrix of 0s. The perfect discriminator will output all 1s for real images and all 0s for fake images. The discriminator loss outputs the average of the real and generated loss."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    def discriminator_loss(real, generated):\n","        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n","\n","        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n","\n","        total_disc_loss = real_loss + generated_loss\n","\n","        return total_disc_loss * 0.5"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The generator wants to fool the discriminator into thinking the generated image is real. The perfect generator will have the discriminator output only 1s. Thus, it compares the generated image to a matrix of 1s to find the loss."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    def generator_loss(generated):\n","        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["We want our original photo and the twice transformed photo to be similar to one another. Thus, we can calculate the cycle consistency loss be finding the average of their difference."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n","        loss1 = tf.reduce_mean(tf.keras.losses.Huber(0.5,reduction=tf.keras.losses.Reduction.NONE)(cycle_model(real_image), cycle_model(cycled_image)))\n","\n","        return LAMBDA * loss1"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["The identity loss compares the image with its generator (i.e. photo with photo generator). If given a photo as input, we want it to generate the same image as the image was originally a photo. The identity loss compares the input with the output of the generator."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    def identity_loss(real_image, translated_image, LAMBDA):\n","        loss = tf.reduce_mean(tf.keras.losses.Huber(0.5,reduction=tf.keras.losses.Reduction.NONE)(tf.nn.avg_pool2d(real_image, ksize=32, strides=16, padding=\"VALID\"), tf.nn.avg_pool2d(translated_image, ksize=32, strides=16, padding=\"VALID\")))\n","        return LAMBDA *  loss"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Train the CycleGAN\n","\n","Let's compile our model. Since we used `tf.keras.Model` to build our CycleGAN, we can just ude the `fit` function to train our model."]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["with strategy.scope():\n","    cycle_gan_model = CycleGan(\n","        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n","    )\n","\n","    cycle_gan_model.compile(\n","        m_gen_optimizer = monet_generator_optimizer,\n","        p_gen_optimizer = photo_generator_optimizer,\n","        m_disc_optimizer = monet_discriminator_optimizer,\n","        p_disc_optimizer = photo_discriminator_optimizer,\n","        gen_loss_fn = generator_loss,\n","        disc_loss_fn = discriminator_loss,\n","        cycle_loss_fn = calc_cycle_loss,\n","        identity_loss_fn = identity_loss\n","    )"],"execution_count":null,"outputs":[]},{"source":["# FID application\n","\n","The FID score is used to evaluate the quality of images generated by generative adversarial networks, and lower scores have been shown to correlate well with higher quality images."],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["fids=[]\n","\n","monet_gen_losses = []\n","monet_disc_losses = []\n","photo_gen_losses = []\n","photo_disc_losses = []\n","cycle_losses = []\n","\n","best_fid = np.inf\n","for epoch in range(1,EPOCHS+1):\n","\n","    print(\"Epoch = \",epoch)    \n","    hist = cycle_gan_model.fit(\n","                full_dataset,\n","                epochs=1,\n","                steps_per_epoch=(max(n_monet_samples, n_photo_samples)//BATCH_SIZE)\n","            ).history\n","    \n","    monet_gen_losses.append(np.array(hist['monet_gen_loss']).mean())\n","    monet_disc_losses.append(np.array(hist['monet_disc_loss']).mean())\n","    photo_gen_losses.append(np.array(hist['photo_gen_loss']).mean())\n","    photo_disc_losses.append(np.array(hist['photo_disc_loss']).mean())\n","    cycle_losses.append(np.array(hist['cycle_loss']).mean())\n","\n","    if epoch > EPOCHS*0.75:\n","        cur_fid = FID(photo_ds,monet_generator,inception_model,mu,sigma)\n","        fids.append(cur_fid)\n","\n","        print(\"Cur #{} FID = {}\\n\".format(epoch,cur_fid))\n","\n","        best_fid = min(cur_fid, best_fid)\n","        print(\"Best FID: \"+str(best_fid))\n","    \n","monet_generator.save('monet_generator_'+str(epoch)+'.h5')"],"execution_count":null,"outputs":[]},{"source":["# Visualize our metric scores"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["sns.set_style(\"darkgrid\")\n","fig = plt.figure(figsize=(15, 6))\n","\n","plt.plot(range(int(np.ceil(EPOCHS*0.75)), EPOCHS), np.array(fids))\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"FID score\")\n","plt.xlim([np.ceil(EPOCHS*0.75), EPOCHS])\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["fig = plt.figure(figsize=(15, 6))\n","\n","monet_loss_plt, = plt.plot(range(1, len(monet_gen_losses)+1), monet_gen_losses, 'b')\n","monet_disc_plt, = plt.plot(range(1,len(monet_disc_losses)+1), monet_disc_losses, 'c')\n","photo_loss_plt, = plt.plot(range(1,len(photo_gen_losses)+1), photo_gen_losses, 'g')\n","photo_disc_plt, = plt.plot(range(1,len(photo_disc_losses)+1), photo_disc_losses, 'r')\n","cycle_loss_plt, = plt.plot(range(1,len(cycle_losses)+1), cycle_losses, 'k')\n","\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","\n","plt.legend([monet_loss_plt, monet_disc_plt, photo_loss_plt, photo_disc_plt, cycle_loss_plt], \n","          ['monet generator loss', 'monet discriminator loss', 'photo generator loss', 'photo discriminator loss', 'cycle consistency loss'],\n","          loc='center left', bbox_to_anchor=(1, 0.5))\n","plt.title('')"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Visualize our Monet-esque photos"]},{"metadata":{"trusted":true},"cell_type":"code","source":["_, ax = plt.subplots(5, 3, figsize=(32, 32))\n","for i, img in enumerate(photo_ds.take(5)):\n","    prediction = monet_generator(img, training=False)\n","    cycledphoto = photo_generator(prediction, training=False)\n","    prediction = (prediction * 127.5 + 127.5)[0].numpy().astype(np.uint8)\n","    cycledphoto = (cycledphoto * 127.5 + 127.5)[0].numpy().astype(np.uint8)\n","\n","    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n","\n","    ax[i, 0].imshow(img)\n","    ax[i, 1].imshow(prediction)\n","    ax[i, 2].imshow(cycledphoto)\n","    ax[i, 0].set_title(\"Input Photo\")\n","    ax[i, 1].set_title(\"Monet-esque\")\n","    ax[i, 2].set_title(\"Cycled Photo\")\n","    ax[i, 0].axis(\"off\")\n","    ax[i, 1].axis(\"off\")\n","    ax[i, 2].axis(\"off\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["_, ax = plt.subplots(5, 3, figsize=(32, 32))\n","for i, img in enumerate(monet_ds.take(5)):\n","    prediction = photo_generator(img, training=False)\n","    cycledphoto = monet_generator(prediction, training=False)\n","    prediction = (prediction * 127.5 + 127.5)[0].numpy().astype(np.uint8)\n","    cycledphoto = (cycledphoto * 127.5 + 127.5)[0].numpy().astype(np.uint8)\n","\n","    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n","\n","    ax[i, 0].imshow(img)\n","    ax[i, 1].imshow(prediction)\n","    ax[i, 2].imshow(cycledphoto)\n","    ax[i, 0].set_title(\"Input Monet\")\n","    ax[i, 1].set_title(\"Generated Photo\")\n","    ax[i, 2].set_title(\"Cycled Monet\")\n","    ax[i, 0].axis(\"off\")\n","    ax[i, 1].axis(\"off\")\n","    ax[i, 2].axis(\"off\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["ds_iter = iter(photo_ds)\n","for n_sample in range(8):\n","        example_sample = next(ds_iter)\n","        generated_sample = monet_generator(example_sample)\n","        \n","        f = plt.figure(figsize=(32, 32))\n","        \n","        plt.subplot(121)\n","        plt.title('Input image')\n","        plt.imshow(example_sample[0] * 0.5 + 0.5)\n","        plt.axis('off')\n","        \n","        plt.subplot(122)\n","        plt.title('Generated image')\n","        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n","        plt.axis('off')\n","        plt.show()"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["# Create submission file"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import PIL\n","! mkdir ../images"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["i = 1\n","\n","photo_ds = load_dataset(PHOTO_FILENAMES)\n","photo_ds = photo_ds.map(lambda p: photo_blur(p), num_parallel_calls=AUTOTUNE)\n","photo_ds = photo_ds.batch(1, drop_remainder=True)\n","\n","for img in photo_ds:\n","    prediction = monet_generator(img, training=False)[0].numpy()\n","    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n","    im = PIL.Image.fromarray(prediction)\n","    im.save(\"../images/\" + str(i) + \".jpg\")\n","    i += 1"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import shutil\n","shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}